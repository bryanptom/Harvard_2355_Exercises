{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## ECON 2355 Implementation Exercise 1: Deep Learning Review\n",
        "\n",
        "Welcome to Econ 2355! This first exercise is meant to be a review of some basic Deep Learning concepts, and reminder of some basic python implementation tools.\n",
        "\n",
        "### Notes on the class's implementation exercises in general:\n",
        "\n",
        " - These exercises are still being finalized! If you encounter problems please don't hesitate to reach out: tom_bryan@fas.harvard.edu\n",
        "\n",
        " - You are welcome to download these notebooks and complete them on your local machine, or work on them in colab. If you are hoping to run things on your local machine you will likely want to set up an [Anaconda](https://www.anaconda.com/products/distribution) python environment and run notebooks from either [VS Code](https://code.visualstudio.com/download) or [Jupyter Lab](https://jupyterlab.readthedocs.io/en/stable/getting_started/installation.html). For your future Deep Learning-oriented endevours, knowing how to set up an environment to run the frameworks and libraries discussed here will likely be important, so it might not be a bad idea to try setting things up locally. On the other hand, working in colab is nice for reproducibility purposes--anyone can run and/or debug your code without problems.\n",
        "\n",
        " - Exercises in this class use [PyTorch](https://pytorch.org/get-started/locally/), the [dominant](https://www.assemblyai.com/blog/pytorch-vs-tensorflow-in-2023/) research deep learning python framework. If you have a _compelling_ reason why you wish to become more familiar with another framework, like Tensorflow, reach out and we _may_ be able to accomodate that.\n",
        "\n",
        " - In these exercises we'll try to find the sweet spot between providing so much of the code that the implementation is meaningless and leaving so much that the work is overly tedious. Feedback is appreciated!\n",
        "\n",
        " - To submit the assignements, please save the exercise as a `.ipynb` file named `ECON_2355_Exercise_{n}_{firstname}_{lastname}.ipynb` and submit to the appropriate place in XXXXX  \n",
        "\n",
        " - These exercises are graded as complete/incomplete. _Complete_ is defined as showing effort to complete at least half of the steps.\n",
        "\n",
        " - Many of these exercises are adapted from other courses, tutorials, or other sources. Like any good social scientist, I list those sources, so should you choose there are often other places to look for help/partial solutions. How and when you use those resources are entirely up to you and your learning style. One caveat: outside sources for exercises will likely be less and less common as we progress through the course.  "
      ],
      "metadata": {
        "id": "lbQzJukbwpqm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise Set 1: Deep Learning Basics\n",
        "\n",
        "This exercise has two main parts: The first is optional and gives an overview of PyTorch Tensor syntax. In the second we'll construct some simple neural nets, train them, and use them to approximate a few mathematical functions and predict clusters."
      ],
      "metadata": {
        "id": "Xivlulcn1qwy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Part 1: PyTorch Review\n",
        "\n",
        "Complete this section _only_ if you feel you need or want an intro/reminder of PyTorch tensor syntax and operations. If you already feel comfortable, go ahead and skip to section 1.\n",
        "\n",
        "PyTorch is a python library based on the Torch ML framework. All of this course's labs will involve PyTorch, though the degree that PyTorch is explicitly visible will vary.\n",
        "\n",
        "(Adapted from [BYU Deep Learning](mind.cs.byu.edu/courses/474))"
      ],
      "metadata": {
        "id": "BD5-8R-t5A9q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install torch"
      ],
      "metadata": {
        "id": "94n6hN9gIfGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch #PyTorch is imported like any other python library\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "Vz2TLmlx4_QQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The basic PyTorch data structure is the Tensor. Tensors are $\\geq0$ dimensional structures that hold rectangular arrays of data. Tensors are implemented similarly to NumPy arrays, and much of what can be done in NumPy can also be done in PyTorch, though the syntax may difer.\n",
        "\n",
        "Here you will work through several Tensor tasks, each asking you to preform a different manipulation on a tensor. Throughout this course you will need to be comfortable looking up [documentation for PyTorch](https://pytorch.org/docs/stable/index.html) and other (often much less well documented) libraries. Practice by looking up the needed syntax for each operation. Documentation for the first two tasks are provided in the hints, you will need to find the rest."
      ],
      "metadata": {
        "id": "HTTuS_Nm6tFL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 1\n",
        "\n",
        "Construct a 5x10 tensor named `a` of ones, and of dtype `long`\n",
        "[hint](https://pytorch.org/docs/stable/generated/torch.ones.html)\n",
        "\n",
        "Print out `a.size()` to verify the tensor is the correct shape."
      ],
      "metadata": {
        "id": "8U3DR3gZ8Yl2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.ones((5, 10), dtype = torch.long)\n",
        "a.size()"
      ],
      "metadata": {
        "id": "o-_Toi7o6LlJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 2\n",
        "\n",
        "Knowing how to manipulate Tensor dimensions is frequently important.\n",
        "\n",
        "Change `a` from a 5x10 tensor into a 5x1x10 tensor. [hint](https://pytorch.org/docs/stable/generated/torch.unsqueeze.html)\n",
        "\n",
        "Print out the `size` again to verify the change worked as expected."
      ],
      "metadata": {
        "id": "K8MzwtCA9SbF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.unsqueeze(a, 1)\n",
        "a.size()"
      ],
      "metadata": {
        "id": "17gySICW1phL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 3\n",
        "\n",
        "Swap transpose `a` along several axes, making it 10x5x1 tensor"
      ],
      "metadata": {
        "id": "IQHKG1LZ-B5u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sGjr1Btdwcym"
      },
      "outputs": [],
      "source": [
        "a = torch.transpose(a, 0, 2)\n",
        "a = torch.transpose(a, 1, 2)\n",
        "a.size()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 4\n",
        "\n",
        "Remove the third dimension, making `a` a 10x5 tensor."
      ],
      "metadata": {
        "id": "FQy_kEyL-Tmk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.squeeze(a, -1)\n",
        "a.size()"
      ],
      "metadata": {
        "id": "0zWTnlHE-3ag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 5\n",
        "\n",
        "Preform a series of mathematical operations on `a`, as indicated in the comments"
      ],
      "metadata": {
        "id": "MZTex0DUwnF4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Multiply all values in a by the constant 2\n",
        "a = a * 2\n",
        "\n",
        "# Add 6 to all values in a\n",
        "a += 6\n",
        "\n",
        "# Subtract 4 from the value at the 2nd row, 3rd column of a\n",
        "a[2, 3] = a[2, 3] - 4\n",
        "\n",
        "# Divide all values in a's 4th row by 3\n",
        "a[4, :] = a[4, :] / 3\n",
        "\n",
        "# Create a second tensor of ones, b, also 10x5.\n",
        "b = torch.ones((10, 5), dtype=torch.long)\n",
        "\n",
        "# Add add b to a\n",
        "a += b\n",
        "\n",
        "# Create a tensor of random numbers named c,\n",
        "# with each number drawn from the uniform distribution\n",
        "# on [0, 1). Make this tensor's shape 5x8.\n",
        "c = torch.rand((5, 8))\n",
        "\n",
        "# Tensors often cannot preform mathematical operations with tensors\n",
        "# of other data types. Change a's type to float\n",
        "a = a.float()\n",
        "\n",
        "# Matrix multiply a and c, call the result d\n",
        "d = a @ c\n",
        "\n",
        "# Flatten d into a one dimensional array\n",
        "d = d.flatten()\n",
        "\n",
        "# Turn d into a numpy array\n",
        "d = d.numpy()\n",
        "\n",
        "# Turn d back into a tensor\n",
        "d = torch.from_numpy(d)\n",
        "\n",
        "# Print d's shape to verify all is right\n",
        "d.size()"
      ],
      "metadata": {
        "id": "f6Pv7-it_Jpr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 6\n",
        "\n",
        "Tensor operations are much faster when run on a GPU. Colab gives access to GPUs in its free version. If you are working through this exercise on a local machine and _do not_ have GPU access, skip this step for now. Future exercises will require GPU access, however.\n",
        "\n",
        "If you are working through this exercise on a local machine and _do_ have GPU access, the following may be instructive (if using Jupyter):\n",
        "https://cschranz.medium.com/set-up-your-own-gpu-based-jupyterlab-e0d45fcacf43\n",
        "\n",
        "If you are working through this exercise in Google Colab, running on a GPU is easy! Go to Runtime->Change Runtime Type and change \"Hardware Accelerator\" to GPU. You will have to restart the runtime (meaning you will lose all saved variables and will need to rerun the above lines) when this change occurs.\n",
        "\n",
        "For this task, complete the GPU-related tasks in the comments"
      ],
      "metadata": {
        "id": "HA39xJ5dBS7b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify that a GPU is available\n",
        "assert torch.cuda.is_available()\n",
        "\n",
        "# Create a:\n",
        "a = torch.ones((8, 10), dtype=torch.long)\n",
        "\n",
        "# Move a onto the gpu\n",
        "a = a.cuda()\n",
        "\n",
        "# Create a tensor of ones named b, the same size as a.\n",
        "# Initialize b on the GPU (do this in one line)\n",
        "b = torch.ones_like(a)\n",
        "\n",
        "# Operations between tensors will only work if they are\n",
        "# on the same device. Add b to a\n",
        "a += b\n",
        "\n",
        "# Tensors can (mostly) only be converted to other formats\n",
        "# from the CPU. Move a back to the cpu and convert it to a\n",
        "# numpy array\n",
        "a = a.cpu().numpy()\n",
        "\n",
        "assert isinstance(a, np.ndarray)"
      ],
      "metadata": {
        "id": "NYbEzhpNAR-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We've explored a relatively small portion of the [possible tensor operations](https://pytorch.org/docs/stable/tensors.html), but hopefully this has helped to introduce PyTorch/jog your memory!\n",
        "\n",
        "With that that knowledge at hand, we're ready to use PyTorch to create a first Neural Net."
      ],
      "metadata": {
        "id": "ipKg0EeHF3sp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Part 2: Deep Learning Basics\n",
        "\n",
        "These exercises ask you to apply PyTorch gradient descent to create models based data in two different cases. Along the way you will also take a more in-depth look at the backpropagation process."
      ],
      "metadata": {
        "id": "U4iilVYXGQlg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### a) **Parameter Estimation**\n",
        "\n",
        "(Adapted from [PyTorch Tutorials](https://pytorch.org/tutorials/beginner/pytorch_with_examples.html))\n",
        "\n",
        "In this first example we directly use Newton's Method to estimate the parameters of a fouth degree polynomial that approximates the function $f(x) = sin(x)$ over the period $[-2\\pi, 2\\pi]$.\n",
        "\n",
        "I.e. given $p(x) = ax^3 + bx^2 + cx + d$, we want to find $a, b, c,$ and $d$ such that $\\int_{-2\\pi}^{2\\pi} (f(x) - p(x))^2 dx$ is minimized.  \n",
        "\n",
        "While we will not use Deep Learning to find those parameters _per se_, this example is extremely instructive because it illustrates the general framework of any _supervised_ Deep Learning problem. I/e. any problem where we can find/create labels on known data and want to make predictions about unseen data. In a simplified form, most problems are solved by:\n",
        "1. Finding/making labeled training data for a problem.\n",
        "2. Determining the format/shape of the model you will use to make predictions. Set initial parameter values.\n",
        "3. Successively estimating optimal model parameters via repeated backpropagation.\n",
        "4. Evaluate to estimated model on a validation set.\n",
        "5. Repeat (2-4) with a variety of model forms and hyperparmameters to find an optimal solution.\n",
        "\n",
        "The following steps will walk through these five steps for the problem stated above:"
      ],
      "metadata": {
        "id": "0N-DQIX9JXeA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import math\n",
        "from matplotlib import pyplot as plt"
      ],
      "metadata": {
        "id": "tZYEQ5e1JRcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, create training data:"
      ],
      "metadata": {
        "id": "xUX5Z3cEMEhE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set X as a tensor with 2000 evenly spaced values between -2pi and 2pi\n",
        "X = torch.linspace(-math.pi, math.pi, 2000)\n",
        "# Set Y as a tensor with the function f(x) = sin(x) at each value of X\n",
        "Y = torch.sin(X)"
      ],
      "metadata": {
        "id": "7dZQ1UnjJS-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And then plot the target function"
      ],
      "metadata": {
        "id": "UCNdwM-5MyLF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(X, Y)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nQ0OfoYiIkkA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Start with parameters initialized to random values\n",
        "a, b, c, d = torch.randn(()), torch.randn(()), torch.randn(()), torch.randn(())"
      ],
      "metadata": {
        "id": "hdLSExGC8NVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "An important parameter in gradient descent is the step size, commonly called the \"Learning Rate.\" Often the optimal learning rate will need to be empirically determined. Choose a value here--you may need to come back and update later!"
      ],
      "metadata": {
        "id": "T_2-0Q2ANSec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 1e-6"
      ],
      "metadata": {
        "id": "LU3GrfY15W_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll now set up a _training loop_ to estimate our model. Training loops implement part (3) of the above recipie: estimating optimal model parameters via repeated backpropagation. The essential steps are:\n",
        "\n",
        "- For n _epochs_ do:\n",
        "  - Get the model's output at each data point (The _forward pass_)\n",
        "  - Compare the model's outputs to the correct values using a _loss function_\n",
        "  - Use backpropagation to find the model's gradient for each parameter (The _backward pass_)\n",
        "  - Update parameters according to their gradients\n",
        "\n",
        "In the next section, fill in the indicated lines to use these steps to optimize the parameters in our toy example. We will use Mean Squared Error (MSE) as the loss function:\n",
        "\n",
        "$$L(y, \\hat{y}) = \\sum_{i=1}^n (\\hat{y}_i - y_i)^2$$\n",
        "\n",
        "To compute the gradients (really just the derivatives in this case) with respect to the loss, remember that the chain rule applies here, so we will have:\n",
        "\n",
        "$$\\frac{dL}{d\\hat{y}} = 2(\\hat{y}_i - y_i)$$\n",
        "\n",
        "Which then gives:\n",
        "\n",
        "$$\\frac{dL}{da} = \\frac{dL}{d\\hat{y}} \\frac{d\\hat{y}}{da} = \\frac{dL}{d\\hat{y}} \\frac{dp(\\hat{x})}{da} = 2(\\hat{y}_i - y_i)\\hat{x}^4$$\n",
        "\n"
      ],
      "metadata": {
        "id": "gasgP1Wr5mP6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 1000\n",
        "losses = []\n",
        "\n",
        "for _ in range(n_epochs):\n",
        "\n",
        "  # Run each value through the model (we do this as a batch)\n",
        "  y_preds = a * X**3 + b * X**2 + c * X + d\n",
        "\n",
        "  # TOOD: Compute the loss according to the formula above\n",
        "  loss = (y_preds - Y).pow(2).sum().item()\n",
        "  losses.append(loss)\n",
        "\n",
        "  # Compute the gradient of y_preds with respect to the loss\n",
        "  grad_y_preds = 2.0 * (y_preds - Y)\n",
        "\n",
        "  # Compute the gradient with respect to a\n",
        "  grad_a = (grad_y_preds * X**3).sum()\n",
        "\n",
        "  # TODO: Compute gradient with respect to the other parameters\n",
        "  grad_b = (grad_y_preds * X**2).sum()\n",
        "  grad_c = (grad_y_preds * X**1).sum()\n",
        "  grad_d = (grad_y_preds).sum()\n",
        "\n",
        "  # Update a with gradient descent\n",
        "  a -= learning_rate * grad_a\n",
        "\n",
        "  # TODO: Update the other parameters using gradient descent\n",
        "  b -= learning_rate * grad_b\n",
        "  c -= learning_rate * grad_c\n",
        "  d -= learning_rate * grad_d\n"
      ],
      "metadata": {
        "id": "KuF1HpNr5i5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great! Now let's check how we did by plotting the estimated curve"
      ],
      "metadata": {
        "id": "eJStrxwyKKY-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(X, Y, label='f(x) = sin(x)')\n",
        "plt.plot(X, a * X**3 + b * X**2 + c * X + d, label = 'Estimated params')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hkf5xME4IWwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the solution doesn't look quite right, or if the parameter values are `nan`, try adjusting the learning rate above! Hint: something $\\leq 10^{-6}$ may be needed."
      ],
      "metadata": {
        "id": "iG0sM-YNPWM8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### b) **Parameter Estimation in PyTorch Style**\n",
        "\n",
        "(Also adapted from [PyTorch Tutorials](https://pytorch.org/tutorials/beginner/pytorch_with_examples.html))\n",
        "\n",
        "In this example we will repeat the same exercise as above, but in a more PyTorch-y style. Most of this is filled in for you, as an instructive example. First, let's mention a few key PyTorch objects:\n",
        "\n",
        "1. Modules (`torch.nn.module`) are the base class for all neural network models. All models are a subclass of `module`. In addition, most sub-parts of models you create will be subclasses of `module`. Subclasses must implement a few key functions:\n",
        "  - `__init__` constructs the model\n",
        "  - `forward` Defines the way a module processes data to create an output\n",
        "  - `backward` Computes the gradient for each parameter in the model.\n",
        "  - Note: the `__call__` method in `module` calls `forward`\n",
        "\n",
        "2. DataLoaders (`torch.utils.data.DataLoader`) combines two things:\n",
        "  - A Dataset (`torch.utils.data.Dataset`)\n",
        "  - A Sampler (` torch.utils.data.Sampler`) Note, the sampler is often ommited, since default behavior is usually sufficent\n",
        "To produce an interatable dataset batched in an appropriate way\n",
        "\n",
        "In this section we will re-implement the earlier example with these new objects"
      ],
      "metadata": {
        "id": "PliXP_21QRA0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Redefine our X and Y data\n",
        "x = torch.linspace(-math.pi, math.pi, 2000)\n",
        "y = torch.sin(x)\n",
        "\n",
        "## First, in this example it is easier to precompute the powers of x. We create a (2000x3) tensor 'xx' with\n",
        "# [ [ x^1 ],\n",
        "#   [ x^2 ],\n",
        "#   [ x^3 ] ]\n",
        "#\n",
        "p = torch.tensor([1, 2, 3])\n",
        "xx = x.unsqueeze(-1).pow(p)"
      ],
      "metadata": {
        "id": "3V0zEFFJQEPt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we will set up our model. A few useful PyTorch sub modules for this exercise:\n",
        "\n",
        " - Linear Layers ([`torch.nn.Linear`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)) are fully connected layers. They apply the transformation $y = xA^T + b$ where $x$ is input data of dimension $(w_i, h_i)$, $A$ is a matrix of weights with dimensions $(w_j, h_j)$, and $b$ is a matrix of biases with dimension $(w_i, w_j)$. The layer is initialized by calling `torch.nn.Linear(w_i, w_j)`\n",
        " - Flatten Layer ([`torch.nn.Flatten`](https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html)) flattens a tensor by eliminating $\\geq1$ dimension and \"flattening\" the data into the remaining dimensions."
      ],
      "metadata": {
        "id": "uu-3em9aU71k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##  This model runs a linear layer and then a flatten layer sequentially.\n",
        "\n",
        "class PolyModel(torch.nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(PolyModel, self).__init__()\n",
        "    self.linear = torch.nn.Linear(3, 1)\n",
        "    self.flatten = torch.nn.Flatten(0, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.flatten(self.linear(x))"
      ],
      "metadata": {
        "id": "CJ69TtbQP-Xq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will make the DataLoader for this dataset. First we need to produce a pytorch dataset from `X` and `Y`. A guide to creating a custom dataset can be found [here](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#creating-a-custom-dataset-for-your-files)."
      ],
      "metadata": {
        "id": "hRf1jsOhXZn1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PolyDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, X, y):\n",
        "    self.X = X\n",
        "    self.y = y\n",
        "    self.len = self.X.shape[0]\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return self.X[index], self.y[index]\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.len"
      ],
      "metadata": {
        "id": "bZwVBBzIYQjw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we need to create a coresponding DataLoader object, which will serve up (X, y) pairs. In the DataLoader we need to specify a _batch size_, which indicated how many pairs are served simultaneously. We could serve all at once, as in the previous example, but here we'll just use a more common value, 64."
      ],
      "metadata": {
        "id": "oaDWzaUgaDWY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = PolyDataset(xx, y)\n",
        "poly_dataloader = torch.utils.data.DataLoader(dataset=data, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "id": "kxmtJMxMaX49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the previous problem we defined our Loss Function by hand. PyTorch has a wide variety of Loss Functions already implemented, and we can use one of them here:"
      ],
      "metadata": {
        "id": "t59go0rmbDfg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = torch.nn.MSELoss()"
      ],
      "metadata": {
        "id": "T0GTORtZbTAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A final piece of the PyTorch training loop is the Optimizer, which is responsible for updating the parameters of our model. One common Optimizer is the Stochastic Gradient Descent optimizer (`torch.optim.SGD`). An optimizer is constructed by passing it the model parameters we want it to optimize for us."
      ],
      "metadata": {
        "id": "194OfCdWbXaE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = PolyModel()\n",
        "learning_rate = 1e-4\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)"
      ],
      "metadata": {
        "id": "uDF-LnUQbvA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can create a more PyTorch-y training loop for the problem. It follow the same basic format as before, but without many of the tedious steps because of the PyTorch objects being used."
      ],
      "metadata": {
        "id": "qe3wdAPoao-T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# TODO: fill in the missing lines in the training loop\n",
        "losses = []\n",
        "n_epochs = 500\n",
        "\n",
        "for _ in tqdm(range(n_epochs)):\n",
        "\n",
        "  for X, Y in poly_dataloader:\n",
        "\n",
        "    # Create predictions by running the model\n",
        "    preds = model(X)\n",
        "\n",
        "    # Compute the loss with the loss function defined above\n",
        "    loss = loss_fn(preds, Y)\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    # Zero the optimizer grads by calling optimizer.zero_grad()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Call the `backward` function on the loss to compute the gradients\n",
        "    loss.backward()\n",
        "\n",
        "    # Call `step` on the optimzer to update the parameters\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "id": "ue0KP73Na289"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When the training loop is finished, we can again compare the two functions"
      ],
      "metadata": {
        "id": "qnn48lEedswP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  model_preds = model(xx).detach().numpy()\n",
        "\n",
        "plt.plot(x, y, label='f(x) = sin(x)')\n",
        "plt.plot(x, model_preds, label = 'Estimated params')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "szKlDekLd5R0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### c) **Approximting a nonlinear function**\n",
        "\n",
        "(Adapted from [datacamp](https://www.datacamp.com/tutorial/pytorch-tutorial-building-a-simple-neural-network-from-scratch))\n",
        "\n",
        "In this exercise we will train a simple neural net to predict which nonlinearlly-seperated cluster a data point belongs too. Here is the dataset we will be using:"
      ],
      "metadata": {
        "id": "qEK_tWaHfjzG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "from matplotlib import pyplot as plt\n",
        "X, y = make_blobs(n_samples = 1000, n_features = 2, centers = 4, random_state = 20)\n",
        "plt.scatter(X[:,0], X[:,1], c=y)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "w7KxGByghJsP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this exercise you will need to:\n",
        "\n",
        "1. Create a Dataset class and associated DataLoader that serves up X, y pairs\n",
        "2. Create a custom neural model with three layers:\n",
        "  - An 2-d input layer (the original data)\n",
        "  - A 10-d hidden layer (fully connected)\n",
        "  - A n-d (where n=number of blobs, 4 in this case) output layer\n",
        "\n",
        "  The model will also have two _activation_ functions: a nonlinear `ReLu` function on the hidden layer and a `softmax` function on the output layer.\n",
        "\n",
        "3. Initialize a loss function and optimizer for this problem\n",
        "\n",
        "4. Create and run a training loop (similar to above) to teach the model how to predict clusters.\n",
        "\n",
        "5. Evalute the model's preformance.\n",
        "\n",
        "We provide some code, but you will need to do most on your own, following previous examples."
      ],
      "metadata": {
        "id": "MdZXyhDVhzmC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Create Datasets and DataLoaders for this dataset**\n"
      ],
      "metadata": {
        "id": "nzN1UsANkd7z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "### Split the data into a test and training set: 80% train and 20% test\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
        "### Create a custom dataset class for this data, following the example above\n",
        "class Data(torch.utils.data.Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.from_numpy(X.astype(np.float32))\n",
        "        self.y = torch.from_numpy(y.astype(np.float32))\n",
        "        self.len = self.X.shape[0]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.X[index], self.y[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "# Initialize a train dataset and associated train dataloader\n",
        "train_data = Data(X_train, y_train)\n",
        "train_dataloader = torch.utils.data.DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Initialize a train dataset and associated train dataloader. Use batch_size = 1.\n",
        "test_data = Data(X_test, y_test)\n",
        "test_dataloader = torch.utils.data.DataLoader(dataset=test_data, batch_size=1, shuffle=True)"
      ],
      "metadata": {
        "id": "kgu2Wofbkj5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Create a neural model for this data**"
      ],
      "metadata": {
        "id": "Qkv9bAbulkmK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomModel(torch.nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(CustomModel, self).__init__()\n",
        "    # Create a linear layer from 2 to 10 dimensions\n",
        "    self.linear_one = torch.nn.Linear(2, 10)\n",
        "\n",
        "    # Create a ReLU activation layer\n",
        "    self.relu = torch.nn.ReLU()\n",
        "\n",
        "    # Create a linear layer from 10 to 4 dimensions\n",
        "    self.linear_two = torch.nn.Linear(10, 4)\n",
        "\n",
        "    # Create a softmax layer\n",
        "    self.softmax = torch.nn.Softmax()\n",
        "\n",
        "  # Write the forward method to apply each of the layers sequentially\n",
        "  def forward(self, x):\n",
        "    return self.softmax(self.linear_two(self.relu(self.linear_one(x))))"
      ],
      "metadata": {
        "id": "L516jGYHki-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Initialize Loss and Optimizer**\n",
        "\n",
        "In this problem we will use [Cross Entropy Loss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html). Intunitively, Cross Entropy Loss \"rewards\" model that produce a high value corresponding to the correct class and very low values for the other classes."
      ],
      "metadata": {
        "id": "kQVVZmrXmu7V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.1\n",
        "\n",
        "## Initialize Cross Entropy Loss Function\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "## Initializer the model and optimizer\n",
        "model = CustomModel()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n"
      ],
      "metadata": {
        "id": "GMuDld64mq7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Create and run a training loop for this problem. See above for an example**"
      ],
      "metadata": {
        "id": "EI7e70hSpZqi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# TODO: fill in the missing lines in the training loop\n",
        "\n",
        "n_epochs = 100\n",
        "\n",
        "for _ in tqdm(range(n_epochs)):\n",
        "\n",
        "  for X, y in train_dataloader:\n",
        "\n",
        "    # Create predictions by running the model\n",
        "    preds = model(X)\n",
        "\n",
        "    # Compute the loss with the loss function defined above\n",
        "    loss = loss_fn(preds, y.long())\n",
        "\n",
        "    # Zero the optimizer grads by calling optimizer.zero_grad()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Call the `backward` function on the loss to compute the gradients\n",
        "    loss.backward()\n",
        "\n",
        "    # Call `step` on the optimzer to update the parameters\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "id": "grHtgZ90pPFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Evaluate the Model**\n",
        "\n",
        "Run the model on each value in your test dataloader. How does the model do? We use `torch.no_grad` to tell the model not to track gradients for these observations"
      ],
      "metadata": {
        "id": "WBogG_idp8Lv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_correct = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "  for X, y in test_dataloader:\n",
        "    # Check if the model has predicted accurately\n",
        "    output = model(X)\n",
        "    pred = np.argmax(output)\n",
        "    if pred == y.item():\n",
        "      n_correct += 1\n",
        "\n",
        "print(n_correct / len(test_data))"
      ],
      "metadata": {
        "id": "OQ2gMz5_pmJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I2jVvaA4-GOV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}