{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32E2Z8HQsdMX"
      },
      "source": [
        "## ECON 2355 Implementation Exercise 3: Transformers and Language Models\n",
        "\n",
        "This exercise has just one part:\n",
        " - **1: Large Language Models for Language Understanding**: This task applies several modern Language models to a sentiment classification task, demonstrating downstream applications of these large language models. It also introduces the popular [huggingface](https://huggingface.co/) framework, an extremely popular API for working with these models, similar to the timm library introduced in the previous exercises.    \n",
        "\n",
        "### Notes on the class's implementation exercises in general:\n",
        "\n",
        " - These exercises are still being finalized! If you encounter problems please don't hesitate to reach out: tom_bryan@fas.harvard.edu\n",
        "\n",
        " - You are welcome to download these notebooks and complete them on your local machine, or work on them in colab. If you are hoping to run things on your local machine you will likely want to set up an [Anaconda](https://www.anaconda.com/products/distribution) python environment and run notebooks from either [VS Code](https://code.visualstudio.com/download) or [Jupyter Lab](https://jupyterlab.readthedocs.io/en/stable/getting_started/installation.html). For your future Deep Learning-oriented endevours, knowing how to set up an environment to run the frameworks and libraries discussed here will likely be important, so it might not be a bad idea to try setting things up locally. On the other hand, working in colab is nice for reproducibility purposes--anyone can run and/or debug your code without problems.\n",
        "\n",
        " - Exercises in this class use [PyTorch](https://pytorch.org/get-started/locally/), the [dominant](https://www.assemblyai.com/blog/pytorch-vs-tensorflow-in-2023/) research deep learning python framework. If you have a _compelling_ reason why you wish to become more familiar with another framework, like Tensorflow, reach out and we _may_ be able to accomodate that.\n",
        "\n",
        " - In these exercises we'll try to find the sweet spot between providing so much of the code that the implementation is meaningless and leaving so much that the work is overly tedious. Feedback is appreciated!\n",
        "\n",
        " - To submit the assignements, please save the exercise as a `.ipynb` file named `ECON_2355_Exercise_{n}_{firstname}_{lastname}.ipynb` and submit to the appropriate place in XXXXX  \n",
        "\n",
        " - These exercises are graded as complete/incomplete. _Complete_ is defined as showing effort to complete at least half of the steps.\n",
        "\n",
        " - Many of these exercises are adapted from other courses, tutorials, or other sources. Like any good social scientist, I list those sources, so should you choose there are often other places to look for help/partial solutions. How and when you use those resources are entirely up to you and your learning style. One caveat: outside sources for exercises will likely be less and less common as we progress through the course.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9K2TOhVpwGL3"
      },
      "source": [
        "### Exercise Set 3: Intro to Large Language Models and Sentiment Classification with BERT\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGaep-rAtVNb"
      },
      "source": [
        "### 1. Intro to Large Language Models\n",
        "\n",
        "This set of exercises introduces the huggingface `transformers` library, which provides a wide variety of pretrained language models. Pretrained language models have (hopefully) grasped much of language's structure and composition via their training procedure and can quickly generalize to other tasks.\n",
        "\n",
        "This is very similar to how vision models are pretrained on ImageNet, which gives them a broad understanding of how vision works and what features in an image might be important to making determinations about that image. In the same way, language models know how to break text down into its salient features.\n",
        "\n",
        "Let's start by installing the HuggingFace `transformers` library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EIrcShnU1qzd"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k12N5wUHuyhc"
      },
      "source": [
        "And some necessary imports:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wEuA3RbpuyPp"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from PIL import Image\n",
        "import torchvision\n",
        "from matplotlib import pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from transformers import pipeline, BertTokenizer, BertModel, BertForPreTraining"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMP0kogH2PKs"
      },
      "source": [
        "We've talked a good amount about BERT in the course, so you likely know that it is a masked language model. In essence, it is trained to predict the token masked by a `[MASK]` token in a given sentence.\n",
        "\n",
        "You can see a good example of this task by using the `fill-mask` pipeline option, as shown here. Calling `unmasker` on a sentence containing the `[MASK]` token will provide the five most likely candidates for the masked word."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6z7tedUOk7n"
      },
      "outputs": [],
      "source": [
        "unmasker = pipeline('fill-mask', model='bert-base-uncased')\n",
        "unmasker('BERT is a [MASK] language model.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ybrh2MDs24AE"
      },
      "source": [
        "Making predictions like this one of the two essential tasks of BERT. It provides the majority of its token-level understanding. As it learns to infer the masked word, it learns how language is used in context. Feel free to experiment by using different sentences and masking different characters. Are there situations where BERT does better or worse?\n",
        "\n",
        "Experimenting with these masked examples is also a good way to see some of the biases or inaccuracies enbedded in the model. Try, for example, the input sentence\n",
        "\n",
        "\n",
        "```\n",
        "My mother worked hard as a [MASK] to feed our family.\n",
        "```\n",
        "compared with\n",
        "```\n",
        "My father worked hard as a [MASK] to feed our family.\n",
        "```\n",
        "\n",
        "Note that the biases reflected in responses like this do not reflect a problem with the model _per se_, but with the training data provided to the model. BERT and models like it use words in similar contexts as they've seen previously. In this case, BERT has consistently seen \"father\" associated with historically masculine professions and \"mother\" with historically feminine ones.\n",
        "\n",
        "This also explains apparent social, moral, or political stances the model favors. See, for example, responses to:\n",
        "\n",
        "```\n",
        "Capitalism is a [MASK] idea.\n",
        "Socialism is a [MASK] idea.\n",
        "Communism is a [MASK] idea.\n",
        "```\n",
        "\n",
        "Of course, these are just annecdotal examples, although others ([1](https://arxiv.org/pdf/2004.09456.pdf), [2](https://arxiv.org/pdf/2010.00133.pdf)) have proposed methods to quantify this bias."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ek4j1zfAfzUG"
      },
      "source": [
        "##### **Applying BERT to Sentiment Classification**\n",
        "\n",
        "In the following exercises, we will use BERT to predict sentiments coresponding to tweets about US airlines. This [dataset](https://www.kaggle.com/datasets/crowdflower/twitter-airline-sentiment) comes from [Kaggle](https://www.kaggle.com/), a good repository for example datasets like this.\n",
        "\n",
        "We will teach BERT to predict sentiments by finetuning it on this particular task. In particular, we will train the model to adapt its [CLS] token to predict whether a tweet is positive, negative, or neutral in its stance towards airlines.\n",
        "\n",
        "First, let's bring in the data and show some examples. **Important:** You will need a Kaggle account to access this data, which is a good thing to have.In general. Once you've made an account, you will need to follow the instructions [here](https://www.kaggle.com/docs/api) under **Authentication** to generate a Kaggle API key. Upload the key (the file called `kaggle.json` to colab in the default location (under `content/`) and then run the following cell to download the relevant data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6h8IEA7hAEB"
      },
      "source": [
        "##### a) **Data and Examples**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yXaCPW9B2o9m"
      },
      "outputs": [],
      "source": [
        "!mkdir /root/.kaggle\n",
        "!cp /content/kaggle.json /root/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d crowdflower/twitter-airline-sentiment -p ./airline_tweets\n",
        "!unzip /content/airline_tweets/twitter-airline-sentiment.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbZ_B91Vi-H4"
      },
      "source": [
        "This dataset comes in csv format, with a series of columns, including the text of the tweet, the sentiment behind the tweet (one of `postive`, `negative`, or `neutral), and a number of other features like the date, airline the tweet refers to, etc. In this example we will use only the text of the tweet to predict its sentiment, so we will discard the remaining features.\n",
        "\n",
        "The following cell will bring the data into memory and show some illustrative examples (the data is originally sorted by airline, so they will all be from Virgin America). Click the Magic Wand symbol in the output to see things in a more readable format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lS5wyr5afx6g"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "tweets = pd.read_csv('Tweets.csv')[['airline_sentiment', 'text']]\n",
        "tweets.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91zCfi2XkkTH"
      },
      "source": [
        "##### b) **Bringing in BERT**\n",
        "\n",
        "Now that we have our text data, we need to get a model! In this project we will use BERT base uncased from huggingface. You can see more information about that model [here](https://huggingface.co/bert-base-uncased), it is essentially the same as the one described in the original [BERT paper](https://arxiv.org/pdf/1810.04805.pdf). This is one of the most common and widely used LLMs, as you can see from the model's download statistics. It's often instructive to try this model on your tasks first to get a benchmark on performance, before trying other, more task-specific models.\n",
        "\n",
        "BERT needs two objects to run: the model and the tokenizer. The tokenizer takes arbitrary text sequences and turns them into vectors for the model, while the model calls BERT's self attention mechanism to create a dense, feature-rich embedding of the sequence.\n",
        "\n",
        "The following cell will bring in both parts of BERT and run it over a sample sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "607aTOXBmfLf"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForPreTraining.from_pretrained(\"bert-base-uncased\")\n",
        "text = \"[CLS] This is a test sentence.\"\n",
        "encoded_input = tokenizer(text, return_tensors='pt')\n",
        "output = model(**encoded_input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrWLHY9OPyWL"
      },
      "source": [
        "The `output` object here contains all the BERT output, which consists of (see [here](https://huggingface.co/docs/transformers/model_doc/bert#transformers.models.bert.modeling_bert.BertForPreTrainingOutput) in this case, though other BERT models will have different output formats) the final hidden states for each token, sentence relationship classification predictions, and a loss metric. You can access each of these parts separately."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bp_jTG6jkcpn"
      },
      "source": [
        "##### c) **The `[CLS]` token**\n",
        "\n",
        "You may recall from the lecture that BERT's pretraining has two parts: first, it attempts to fill in masked tokens, and secondly it attemps to determine if one sentence logically follows another. BERT input is structured like\n",
        "\n",
        "```\n",
        "[CLS] Sentence 1 [SEP] Sentence 2\n",
        "```\n",
        "\n",
        "And BERT attempts to use a classification head over the output of the `CLS` token to predict whether Sentence 2 logically follows Sentence 1. So BERT is tuned to produce meaningful representations for classifying the entire sequence in it's `CLS` space.\n",
        "\n",
        "In a first example, let's see how BERT does at generating predictions based on its original classification task: does one sentence logically follow the other?\n",
        "\n",
        "Run the model we've initialized over some examples, some of which logically follow each other, and some of which do not. How does BERT do? You can access BERT's classification predictions via `output.seq_relationship_logits`.\n",
        "\n",
        "**Note** The model here is not really tuned for classification, and may make incorrect predictions. However, you should see _lower_ values for the first value in `output.seq_relationship_logits` and _higher_ values (they will likely still be negative) for the second value for unrelated sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWR8wDekjd4V"
      },
      "outputs": [],
      "source": [
        "follows = 'The location of the earthquake places it within the vicinity of a triple junction between the Anatolian, Arabian, and African plates. The mechanism and location of the earthquake are consistent with it having occurred in either the East Anatolian Fault zone or the Dead Sea Transform Fault Zone.'\n",
        "does_not_follow = 'The Dead Sea Transform extends north–south from the Red Sea to the Marash Triple Junction where it meets the East Anatolian Fault.' + \\\n",
        "                  'Like certain other upper houses of state and territorial legislatures and the United States Senate, the state Senate can confirm or reject gubernatorial appointments to state departments, commissions, boards, and other state governmental agencies.'\n",
        "\n",
        "# Encode both sentences (pass 'add_special_tokens=True' as an argument to the tokenizer)\n",
        "follows_encoded = tokenizer(follows, return_tensors='pt', add_special_tokens=True)\n",
        "not_follows_encoded = tokenizer(does_not_follow, return_tensors='pt', add_special_tokens=True)\n",
        "\n",
        "# Run the model over the tokenized text\n",
        "follows_output = model(**follows_encoded)\n",
        "not_follows_output = model(**not_follows_encoded)\n",
        "\n",
        "# Check the outputs and classification scores\n",
        "print('Follows predictions: {}'.format(follows_output.seq_relationship_logits))\n",
        "print('Does not follow predictions: {}'.format(not_follows_output.seq_relationship_logits))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coQTursvzeqM"
      },
      "source": [
        "##### d) **Adding a classification head**\n",
        "\n",
        "The example above produces predictions (the final `seq_relatioship_logits` pair) because it has a classification head on top. Here we add a classification head for our problem (classifying airline tweet sentiment) on top of a BERT model.\n",
        "\n",
        "Now, instead of using the `BertForPreTraining` model, we will switch to the standard `BertModel`, since we no longer need to access the pretraining behavior.\n",
        "\n",
        "What does a classification head look like? The `CLS` token outputs from the model as a dense, 768-dimensional vector. We will add several linear layers to produce a three-dimensional output, and then add a softmax layer to produce probabilities for each of the classes, positive, negative, and neutral."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "znX6pWlOrlwV"
      },
      "outputs": [],
      "source": [
        "class BertSentimentClassifier(torch.nn.Module):\n",
        "  def __init__(self, bert_model):\n",
        "    super(BertSentimentClassifier, self).__init__()\n",
        "    self.bert_model = bert_model\n",
        "\n",
        "    # TODO: Create a linear layer and softmax to move from a 768 dimensional vector to a three-dimensional probability vector\n",
        "    self.linear_1 = torch.nn.Linear(768, 3)\n",
        "    self.softmax = torch.nn.Softmax()\n",
        "\n",
        "  def forward(self, input_ids, mask):\n",
        "    _, pooled_output = self.bert_model(input_ids = input_ids, attention_mask = mask, return_dict = False)\n",
        "    # TODO: run the linear layer and softmax over the pooled output (a 768-d vector)\n",
        "    return self.softmax(self.linear_1(pooled_output))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m9YaSlfF1rI9"
      },
      "outputs": [],
      "source": [
        "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
        "model = BertSentimentClassifier(bert_model)\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0hGD70_4TR7"
      },
      "source": [
        "##### e) **Preparing to train the model**\n",
        "\n",
        "Now that we have our encoder, custom model, and data all initialized, we need to set up the standard elements for a PyTorch training loop (similar to the first two exercise sets). In particular, we will need to create PyTorch datasts and dataloaders, an optimizer,"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQ3KqOY_Qh0Y"
      },
      "source": [
        "First, we apply a mapping to the data to get numeric classication values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lwx4zFWD4RoG"
      },
      "outputs": [],
      "source": [
        "mapping = {'negative': 0,\n",
        "           'neutral': 1,\n",
        "           'positive': 2}\n",
        "\n",
        "# TODO: Encode the semtiment classifications\n",
        "tweets['enc_sentiment'] = tweets['airline_sentiment'].apply(lambda x: mapping[x])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rrwsb9efQrMx"
      },
      "source": [
        "Next, we need to create a dataset object for this dataset. `X` should be the actual texts, while `y` should be the encoded semtiment values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MdKxbs134FkH"
      },
      "outputs": [],
      "source": [
        "# TODO: Finish implementing the TweetDataset object\n",
        "class TweetDataset:\n",
        "  def __init__(self, tweet_data):\n",
        "    self.x = tweet_data['text']\n",
        "    self.y = tweet_data['enc_sentiment']\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.x.shape[-1]\n",
        "\n",
        "  def __getitem__(self, i):\n",
        "    return self.x[i], self.y[i]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fo0jrglMQ5Jc"
      },
      "source": [
        "Then, as usual, we need to create train, test, and validation sets. We also need to create `torch.utils.data.DataLoader` objects from the various datasets. Let's again use **80% train, 10% validation, and 10% test.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_dflhVne7AY0"
      },
      "outputs": [],
      "source": [
        "tweet_dataset = TweetDataset(tweets)\n",
        "\n",
        "# TODO: Split data into three sets, with\n",
        "train_set, val_set, test_set = torch.utils.data.random_split(tweet_dataset, [.8, .1, .1])\n",
        "\n",
        "# TODO: Create dataloaders for the data\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size = 16, shuffle = True)\n",
        "val_loader = torch.utils.data.DataLoader(val_set, batch_size = 16, shuffle = True)\n",
        "test_loader = torch.utils.data.DataLoader(test_set, batch_size = 1, shuffle = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcYvy5L0RPHf"
      },
      "source": [
        "Now we need to create the remaining objects for our training loop. Move the model to the GPU and create a `CrossEntropyLoss` loss function and an `Adam` optimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O0xW2-fY7GRi"
      },
      "outputs": [],
      "source": [
        "learning_rate = 1e-6\n",
        "\n",
        "#TODO: Move the model to the GPU\n",
        "model = model.cuda()\n",
        "\n",
        "# TODO: create an Adam optimizer for the model\n",
        "optim = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
        "\n",
        "# TODO: create a cross entropy loss function\n",
        "loss_fn = torch.nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdrX_x7SRuY3"
      },
      "source": [
        "##### f) **Training the model**\n",
        "\n",
        "Finally, create a training loop for this model. This will look very similar to previous training loops, with a few exceptions:\n",
        "- Input will have to be tokenized _before_ being moved onto the GPU (since the tokenizer is still on the CPU)\n",
        "- Both the `input_ids` and `attention_mask` attribues of the encoded input will need to be moved to the GPU\n",
        "- Running the model over the input data will need to take both the input ids and attention mask as inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4t1D4oFm8BaH"
      },
      "outputs": [],
      "source": [
        "num_epochs = 5\n",
        "\n",
        "for _ in range(num_epochs):\n",
        "\n",
        "  model.train()\n",
        "  for X, y in tqdm(train_loader):\n",
        "    # TODO: tokenize the input\n",
        "    print(X)\n",
        "    enc = tokenizer(X, padding='max_length', max_length=64, truncation=True, return_tensors='pt')\n",
        "    print(enc.keys())\n",
        "\n",
        "    # TODO: move the encoded input ids, encoded attention mask, and y to the GPU\n",
        "    input_ids = enc.input_ids.cuda()\n",
        "    attention_mask = enc.attention_mask.cuda()\n",
        "    y = y.cuda()\n",
        "\n",
        "    # TODO: Run the model over the input, compute the loss, zero the gradients, backpropagate, and step the optimzer\n",
        "    output = model(input_ids, attention_mask)\n",
        "    loss = loss_fn(output, y)\n",
        "    optim.zero_grad()\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "\n",
        "  break\n",
        "  model.eval()\n",
        "  n_correct = 0\n",
        "  for X, y in tqdm(val_loader):\n",
        "    # TODO: complete the evaulation loop, computing validation accuracy\n",
        "    enc = tokenizer(X, padding='max_length', max_length=64, truncation=True, return_tensors='pt')\n",
        "    input_ids = enc.input_ids.cuda()\n",
        "    attention_mask = enc.attention_mask.cuda()\n",
        "    output = model(input_ids, attention_mask)\n",
        "    preds = np.argmax(output.detach().cpu().numpy(), axis = 1)\n",
        "    n_correct += (preds == y.numpy()).sum()\n",
        "\n",
        "  print(f'Val Accuracy epoch {_ + 1}: {n_correct / len(val_set)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJF-kzGjYrm9"
      },
      "source": [
        "After five epochs, you should see accuracy in the 80-90% range. If you don't consider checking the model you defined earlier for any possible errors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVsVyGZbWbHK"
      },
      "source": [
        "##### g) **A more direct, pretrained and finetuned approach**\n",
        "\n",
        "While we can adapt the general BERT model to just about any task, there are also many models (provided by huggingface or other sites) pretrained for a specific task. In the majority of cases, if you have a widely-used data source (like twitter, in our case) and a broad, often-explored task (like sentiment classification, in our case) there is likely a variety of pretrained models to choose from.\n",
        "\n",
        "In this case we can use the [Twitter-roBERTa for Sentiment Analysis](https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest) model, provided through huggingface by [a team of NLP researchers with the University of Cardiff](https://arxiv.org/pdf/2202.03829.pdf). This model is much easier to work with and can be expected to provide more initial knowledge than the first. It:\n",
        "- Has been pretrained on tweets, unlike BERT\n",
        "- Has already been finetuned to classify sentiments\n",
        "\n",
        "The code below will intialize the model and tokenizer for you:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aP32lMa3VR2C"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "from transformers import AutoTokenizer, AutoConfig\n",
        "\n",
        "MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
        "config = AutoConfig.from_pretrained(MODEL)\n",
        "roberta_model = AutoModelForSequenceClassification.from_pretrained(MODEL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ask8Dh_oY7kw"
      },
      "source": [
        "Inputting a sequence into the model will produce a list of pre-softmax outputs for Negative, Neutral, and Positive. We will need to recreate a custom classifier, but **the only change you should need to make from the earlier classifier** is removing the linear layer (since that is provided in the finetuned model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FyBVZep0Yl0P"
      },
      "outputs": [],
      "source": [
        "# TODO Create a RoBerta sentiment classifer, very similar to the previous custom classifier.\n",
        "class RoBertaSentimentClassifier(torch.nn.Module):\n",
        "  def __init__(self, bert_model):\n",
        "    super(RoBertaSentimentClassifier, self).__init__()\n",
        "    self.bert_model = bert_model\n",
        "    self.softmax = torch.nn.Softmax()\n",
        "\n",
        "  def forward(self, input_ids, mask):\n",
        "    pooled_output = self.bert_model(input_ids = input_ids, attention_mask = mask, return_dict = False)\n",
        "    return self.softmax(pooled_output[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RjvzyHcZ3hv"
      },
      "source": [
        "Now we need to move the model to the GPU, and create an optimizer and loss function again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7u-V-2XOZ0SI"
      },
      "outputs": [],
      "source": [
        "learning_rate = 1e-6\n",
        "model = RoBertaSentimentClassifier(roberta_model)\n",
        "\n",
        "#TODO: Move the model to the GPU\n",
        "model = model.cuda()\n",
        "\n",
        "# TODO: create an Adam optimizer for the model\n",
        "optim = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
        "\n",
        "# TODO: create a cross entropy loss function\n",
        "loss_fn = torch.nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cg3tuTiRaFlN"
      },
      "source": [
        "This model should preform well zero-shot on our data, but we can improve preformance by training a bit more on our specific dataset (so that the model can get used to airline-specific terms, for example).\n",
        "\n",
        "In this last step, you need to recreate the training loop from above. It should be an exact copy-paste. We should make one adjustment, however: run an validation loop _first_ (before doing any training) so that you can measure the model's zero-shot performance. How much does the model improve from that benchmark? Does the model converge faster or slower than the bert-base model? Is final accuracy (after five epochs) higher or lower?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ybTIheMqZ_Nm"
      },
      "outputs": [],
      "source": [
        "num_epochs = 5\n",
        "\n",
        "for _ in range(num_epochs):\n",
        "\n",
        "  model.eval()\n",
        "  n_correct = 0\n",
        "  for X, y in tqdm(val_loader):\n",
        "    # TODO: complete the evaulation loop, computing validation accuracy\n",
        "    enc = tokenizer(X, padding='max_length', max_length=64, truncation=True, return_tensors='pt')\n",
        "    input_ids = enc.input_ids.cuda()\n",
        "    attention_mask = enc.attention_mask.cuda()\n",
        "    output = model(input_ids, attention_mask)\n",
        "    preds = np.argmax(output.detach().cpu().numpy(), axis = 1)\n",
        "    n_correct += (preds == y.numpy()).sum()\n",
        "  print(f'Val Accuracy epoch {_ + 1}: {n_correct / len(val_set)}')\n",
        "\n",
        "  model.train()\n",
        "  for X, y in tqdm(train_loader):\n",
        "    # TODO: tokenize the input\n",
        "    enc = tokenizer(X, padding='max_length', max_length=64, truncation=True, return_tensors='pt')\n",
        "\n",
        "    # TODO: move the encoded input ids, encoded attention mask, and y to the GPU\n",
        "    input_ids = enc.input_ids.cuda()\n",
        "    attention_mask = enc.attention_mask.cuda()\n",
        "    y = y.cuda()\n",
        "\n",
        "    # TODO: Run the model over the input, compute the loss, zero the gradients, backpropagate, and step the optimzer\n",
        "    output = model(input_ids, attention_mask)\n",
        "    loss = loss_fn(output, y)\n",
        "    optim.zero_grad()\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTe_kj4jb8n2"
      },
      "source": [
        "Hopefully this exercise has provided a window into not only sentiment classification, but downstream language tasks in general. The system used here (finding a pretrained model, adapting it to a specific task, and then finetuning) will generalize remarkably well across datasets and language tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 2: Perplexity"
      ],
      "metadata": {
        "id": "jI3-SNegR3AR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Create the get_perplexity function\n",
        "def get_perplexity(tokenizer, model, text = \"Economics is a very interesting topic\"):\n",
        "\n",
        "  # Tokenize the inputs\n",
        "  inputs = tokenizer(text, return_tensors='pt')\n",
        "\n",
        "  # Run the inputs through the model, saving the logits\n",
        "  input_seq = inputs['input_ids']\n",
        "  results = model(input_seq)\n",
        "  logits = results.logits\n",
        "\n",
        "  # Initialize loss at zero\n",
        "  loss = torch.tensor(0.0)\n",
        "\n",
        "  # For each logit index (not including the last one), compute the log of softmax probilities.\n",
        "  # Then, look up the logged softmax probibility of the NEXT (i + 1) token in the input sequence.\n",
        "  # That value is p(x_i | x_<i)\n",
        "  # Sum those values\n",
        "  for i in range(inputs.input_ids.size(-1) - 1):\n",
        "    log_softmax = torch.log(torch.softmax(logits[:, i, :], dim = 1))\n",
        "    sub_loss = log_softmax[:, inputs.input_ids[:, i + 1]][0][0]\n",
        "    loss += sub_loss\n",
        "\n",
        "  # Take the exponential of the negative average of summed likelihoods\n",
        "  perplexity = torch.exp(-1 * loss / (inputs.input_ids.size(-1) - 1))\n",
        "\n",
        "  # Return the result\n",
        "  return perplexity.item()\n"
      ],
      "metadata": {
        "id": "PowEf4PZR32u"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}