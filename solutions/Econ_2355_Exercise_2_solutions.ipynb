{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32E2Z8HQsdMX"
      },
      "source": [
        "## ECON 2355 Implementation Exercise 2: Convolutional Neural Nets\n",
        "\n",
        "This exercise has two main parts:\n",
        " - **1: Convolutions from Scratch**: (_Optional_) This task is designed to build familiarity with Convolutional Neural Networks by implementing a convolutional model from (almost) scratch and building a small model using those layers.\n",
        " - **2: CNNs for Remote Sensing**: (_Required_) This task applies several modern Convolutional Networks (VGG, ResNet, ConvNeXt, and MobileNetv3) to a satelite imagery dataset, comparing and contrasting the various models.   \n",
        "\n",
        "If you are not deeply familiar with Neural Networks, completing Part 1 is strongly advised. An understanding of how these models work at a lower level is crucial to developing intuition around newer developments.  \n",
        "\n",
        "### Notes on the class's implementation exercises in general:\n",
        "\n",
        " - These exercises are still being finalized! If you encounter problems please don't hesitate to reach out: tom_bryan@fas.harvard.edu\n",
        "\n",
        " - You are welcome to download these notebooks and complete them on your local machine, or work on them in colab. If you are hoping to run things on your local machine you will likely want to set up an [Anaconda](https://www.anaconda.com/products/distribution) python environment and run notebooks from either [VS Code](https://code.visualstudio.com/download) or [Jupyter Lab](https://jupyterlab.readthedocs.io/en/stable/getting_started/installation.html). For your future Deep Learning-oriented endevours, knowing how to set up an environment to run the frameworks and libraries discussed here will likely be important, so it might not be a bad idea to try setting things up locally. On the other hand, working in colab is nice for reproducibility purposes--anyone can run and/or debug your code without problems.\n",
        "\n",
        " - Exercises in this class use [PyTorch](https://pytorch.org/get-started/locally/), the [dominant](https://www.assemblyai.com/blog/pytorch-vs-tensorflow-in-2023/) research deep learning python framework. If you have a _compelling_ reason why you wish to become more familiar with another framework, like Tensorflow, reach out and we _may_ be able to accomodate that.\n",
        "\n",
        " - In these exercises we'll try to find the sweet spot between providing so much of the code that the implementation is meaningless and leaving so much that the work is overly tedious. Feedback is appreciated!\n",
        "\n",
        " - To submit the assignements, please save the exercise as a `.ipynb` file named `ECON_2355_Exercise_{n}_{firstname}_{lastname}.ipynb` and submit to the appropriate place in XXXXX  \n",
        "\n",
        " - These exercises are graded as complete/incomplete. _Complete_ is defined as showing effort to complete at least half of the steps.\n",
        "\n",
        " - Many of these exercises are adapted from other courses, tutorials, or other sources. Like any good social scientist, I list those sources, so should you choose there are often other places to look for help/partial solutions. How and when you use those resources are entirely up to you and your learning style. One caveat: outside sources for exercises will likely be less and less common as we progress through the course.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9K2TOhVpwGL3"
      },
      "source": [
        "### Exercise Set 2: CNN from scratch and Transfer Learning with Remote Sensing\n",
        "\n",
        "As mentioned above, this exercise has two parts: The first is optional and guides you through creating a Convolutional Neural Network from scratch, i.e. you will actually implement the Convolution operation, maxpooling, the ReLU activation function, etc.\n",
        "\n",
        "The second part will walk you through a plausible use case for modern convolutional neural networks: classifying land use based on satelite imagery. You will compare multiple ConvNets mentioned in the lecture and readings, and use [Weights and Biases](https://wandb.ai/), a popular Deep Learning experiment tracking/evaluation platform to compare models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISwhhORYyBYA"
      },
      "source": [
        "#### Part 1: Convolutional Neural Network from Scratch (Optional)\n",
        "\n",
        "This section will walk you through creating a Convolutional Neural Network from only basic mathematical operations and matrix math. (And the incredible PyTorch backpropagation framework, of course!) In particular, we'll be recreating a smaller version of the Convolutional Network from the last exercises (the one that classified handwritten images of numbers). We make the model here smaller because we are not able to take advantage of PyTorch's extremely fast operations, so a smaller model allows us to train and infer in a reasonable amount of time.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNxZiDGbIs_K"
      },
      "source": [
        "##### a) **Bringing in Data**\n",
        "\n",
        "We're working with the same data as before, so code to download the data, create the `Dataset` and `Dataloader` objects, and transform the data appropriately is all provided."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2on4Qt22wFWW"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import torch\n",
        "import torchvision\n",
        "from math import sqrt\n",
        "\n",
        "train_data = torchvision.datasets.MNIST('/files/', train=True, download=True)\n",
        "test_data = torchvision.datasets.MNIST('/files/', train=False, download=True)\n",
        "\n",
        "fig = plt.figure()\n",
        "for i in range(6):\n",
        "  plt.subplot(2,3,i+1)\n",
        "  plt.tight_layout()\n",
        "  plt.imshow(train_data[i][0], cmap='gray', interpolation='none')\n",
        "  plt.title(\"Ground Truth: {}\".format(train_data[i][1]))\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "fig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nOsslou_qkxf"
      },
      "outputs": [],
      "source": [
        "#Create the datasets\n",
        "train_dataset = torchvision.datasets.MNIST('/files/', train=True, download=True,\n",
        "                             transform=torchvision.transforms.Compose([\n",
        "                               torchvision.transforms.ToTensor(),\n",
        "                               torchvision.transforms.Normalize(\n",
        "                                 (0.1307,), (0.3081,))\n",
        "                             ]))\n",
        "\n",
        "test_dataset = torchvision.datasets.MNIST('/files/', train=False, download=True,\n",
        "                             transform=torchvision.transforms.Compose([\n",
        "                               torchvision.transforms.ToTensor(),\n",
        "                               torchvision.transforms.Normalize(\n",
        "                                 (0.1307,), (0.3081,))\n",
        "                             ]))\n",
        "\n",
        "train_batch_size = 64\n",
        "test_batch_size = 1\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = train_batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size = test_batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Biu852NoJGma"
      },
      "source": [
        "##### b) **ReLu Layer**\n",
        "\n",
        "We will be creating `torch.nn.Module` subclasses for each of the modules we used in our Convolutional Neural Network. That is, we will need to create our own classes to mimic all of:\n",
        "\n",
        "\n",
        "*   `torch.nn.ReLU`\n",
        "*   `torch.nn.Conv2d`\n",
        "*   `torch.nn.MaxPool2d`\n",
        "*   `torch.nn.Linear`\n",
        "*   `torch.nn.Dropout2d`\n",
        "*   `torch.nn.Softmax`\n",
        "\n",
        "The basic format for these classes is fairly simple: we just need to implement a `__init__` function which saves any necessary layer parameters, and initializes any weights and biases, and we need to define a `forward` function, which will pass data through the layer.\n",
        "\n",
        "This first example is quite simple: we need to create a module that will pass data through a ReLU nonlinear activation function. ReLU is defined as:\n",
        "$$f(x) = max(0, x)$$\n",
        "\n",
        "Since this module doesn't need any additional parameters or stored information, we can leave `__init__` as is. All you need to do is modify `forward` (hint, `torch.maximum` and `torch.zeros_like` may be helpful) to evaluate ReLU over each value in `x`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qzDcE3Kiqogp"
      },
      "outputs": [],
      "source": [
        "class ReluLayer(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(ReluLayer, self).__init__()\n",
        "\n",
        "  def forward(self, x):\n",
        "    # TODO: evaluate ReLU(x)\n",
        "    return torch.maximum(x, torch.zeros_like(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsShvSKw4dlh"
      },
      "source": [
        "##### c) **MaxPool Layer**\n",
        "\n",
        "The MaxPool operation is relatively simple: given a tensor with shape (batch, n_channels, width, height), MaxPool will return a tensor with shape (batch, n_channels, width / size, height / size), where (size x size) square blocks in the last two dimensions have been \"pooled\" by keeping only the maximum value in the pool. For more information, see [`torch.nn.MaxPool2d`](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html).\n",
        "\n",
        "Here, your task is to implement the pooling operation itself. Fill the provided `result` tensor with its appropriate values. Hint: `torch.max` will work, and passing in a dimension argument to assign all batches and channels simultaneously will make things much faster. A naive implementation will work fine for what we need here though."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ST5lLa0Y5ezv"
      },
      "outputs": [],
      "source": [
        "class MaxPoolLayer(torch.nn.Module):\n",
        "  def __init__(self, size):\n",
        "    super(MaxPoolLayer, self).__init__()\n",
        "    self.size = size\n",
        "\n",
        "  def forward(self, x):\n",
        "    if x.size()[-1] % self.size != 0 or x.size()[-2] % self.size != 0:\n",
        "      raise ValueError('Tensor is invalid size! Maxpool Kernel must fit evenly within Tensor')\n",
        "\n",
        "    result = torch.empty(x.size()[0], x.size()[1], x.size()[2] // self.size, x.size()[3] // self.size, device=x.device)\n",
        "\n",
        "    # TODO: assign `result` with the correct values found by taking maxes over sections of x\n",
        "    for xi in range(x.size()[2] // self.size):\n",
        "      for yi in range(x.size()[3] // self.size):\n",
        "        result[:, :, xi, yi] = torch.max(torch.max(x[:, :, xi * self.size : (xi + 1) * self.size, yi * self.size : (yi + 1) * self.size], dim=3)[0], dim=2)[0]\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEAW_bHD6JoO"
      },
      "source": [
        "##### d) **Convolutional Layer**\n",
        "\n",
        "A convolutional layer (see [`torch.nn.Conv2d`](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html)) passes several sliding windows (called \"Kernels\") over a multi-layered tensor, with each kernel creating its own new layer.\n",
        "\n",
        "Intunitively, each kernel will start in the upper left corner of the image, compute element-wise multiplication with the section of the input they cover, and then move one space to the right. They then compute the same element-wise multiplication with that section of the image, and continue until the reach the right side of the image. They then move one space down and repeat until they have covered the entire image.\n",
        "\n",
        "Since running convolutions in a time-efficent way is somewhat complicated, this section is provided."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NGjtdHiuGhOq"
      },
      "outputs": [],
      "source": [
        "class ConvLayer(torch.nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, kernel_size):\n",
        "    super(ConvLayer, self).__init__()\n",
        "    self.kernel_size = kernel_size\n",
        "    self.out_channels = out_channels\n",
        "    self.in_channels = in_channels\n",
        "    self.pad_n = (self.kernel_size - 1) // 2\n",
        "    self.padder = torch.nn.ConstantPad2d(self.pad_n, 0)\n",
        "    bounds = in_channels * kernel_size * kernel_size\n",
        "    self.kernels = torch.nn.Parameter(torch.empty(self.in_channels * self.kernel_size * self.kernel_size, self.out_channels).uniform_(-1 / sqrt(bounds), 1 / sqrt(bounds)))\n",
        "    self.biases = torch.nn.Parameter(torch.empty(self.out_channels).uniform_(-1 / (self.kernel_size ** 2), 1 / self.kernel_size ** 2))\n",
        "\n",
        "  def forward(self, x):\n",
        "    result = torch.empty(x.size()[0], self.out_channels, x.size()[2], x.size()[3], device=x.device)\n",
        "    x = self.padder(x)\n",
        "\n",
        "    for xi in range(self.pad_n, x.size()[2] - (self.pad_n + 2)):\n",
        "      for yi in range(self.pad_n, x.size()[3] - (self.pad_n + 2)):\n",
        "        res = x[:, :, xi: xi + self.kernel_size, yi : yi + self.kernel_size].flatten(start_dim=1) @ self.kernels\n",
        "        result[:, :, xi - self.pad_n, yi - self.pad_n] = res + self.biases\n",
        "\n",
        "    return result[:, :, self.pad_n:-1*self.pad_n, self.pad_n:-1*self.pad_n]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1Vcwt6iZG9k"
      },
      "source": [
        "##### d) **Linear Layer**\n",
        "\n",
        "Linear Layers ([`torch.nn.Linear'](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)) simply multiply their input by a learnable matrix, and add a learnable bias vector. Here you will need to implement the class `LinearLayer` from scratch, by initializing the `weights` tensor as a `torch.nn.Parameter` instance containing a `Tensor` with shape `(in_channels, out_channels)` with initial values drawn randomly from the standard normal distribution (see `torch.randn`). `biases` will need to be similarly initialized, but with shape `(out_channels,)`.\n",
        "\n",
        "In `forward` you will need to appropriately define the behavior for the linear layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EvZyL3S36N9V"
      },
      "outputs": [],
      "source": [
        "class LinearLayer(torch.nn.Module):\n",
        "  def __init__(self, in_channels, out_channels):\n",
        "    super(LinearLayer, self).__init__()\n",
        "    self.weights = torch.nn.Parameter(torch.empty(in_channels, out_channels).uniform_(-1 / sqrt(in_channels), 1 / sqrt(in_channels)))\n",
        "    self.biases = torch.nn.Parameter(torch.empty(out_channels).uniform_(-1, 1))\n",
        "\n",
        "  def forward(self, x):\n",
        "    return x @ self.weights + self.biases"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ll28vw3iZOGM"
      },
      "source": [
        "##### e) **SoftMax \"Layer\"**\n",
        "\n",
        "The softmax function is defined by:\n",
        "\n",
        "\n",
        "![Capture.PNG](data:image/png;base64,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)\n",
        "\n",
        "In the context of this \"layer,\" you will need to compute the softmax over the ten value row of the final, `(batch_size, 10)` dimensional output of the model. Do this explicitly, without using `torch.nn.Softmax` of `torch.nn.F.Softmax`.  Hint: you will want to leverage the `dim` argument of `torch.exp`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DYq0xVjU7Ayi"
      },
      "outputs": [],
      "source": [
        "class SoftMaxLayer(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(SoftMaxLayer, self).__init__()\n",
        "\n",
        "  def forward(self, x):\n",
        "    return (torch.exp(x).T / torch.exp(x).sum(dim=1)).T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tob1LKsqZSED"
      },
      "source": [
        "##### f) **Dropout \"Layer\"**\n",
        "\n",
        "Dropout layers randomly assign some percentage of the parameters in the layer to zero. Here we pass `p` as a parameter to the layer, and in the `forward` function replace that percentage of the values in `x` with zeros. You will need to implement the `forward` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tmxl1GMm76xB"
      },
      "outputs": [],
      "source": [
        "class DropoutLayer(torch.nn.Module):\n",
        "  def __init__(self, p = 0.5):\n",
        "    super(DropoutLayer, self).__init__()\n",
        "    self.p = p\n",
        "\n",
        "  def forward(self, x):\n",
        "    mask = torch.rand(x.size(), device = x.device) > self.p\n",
        "    return (x * mask) * (1 / (1 - self.p))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzYd_q8FhlbB"
      },
      "source": [
        "##### g) **Finishing the Process**\n",
        "\n",
        "Now we need to implement a standard PyTorch training loop for the model using the layers we've created. First, we provide a definintion for the model using the new layers we've defined, which looks very similar to the model we used in the first exercises.\n",
        "\n",
        "The primary difference is that we've made several of the layers slightly smaller, because this model will train much more slowly than the model used in the previous exercises, since we are no longer able to leverage PyTorch's extremely fast built in evaluation and backpropagation algorithms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n_2SF46hTebI"
      },
      "outputs": [],
      "source": [
        "class DigitsModel(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(DigitsModel, self).__init__()\n",
        "\n",
        "    ## 1: First convolutional layer, with additional steps\n",
        "    self.conv1 = ConvLayer(1, 5, 5)\n",
        "    self.maxpool1 = MaxPoolLayer(2)\n",
        "    self.relu1 = ReluLayer()\n",
        "\n",
        "    ## 2: Second convolutional layer, with additional steps\n",
        "    self.conv2 = ConvLayer(5, 5, 5)\n",
        "    self.dropout1 = DropoutLayer()\n",
        "    self.maxpool2 = MaxPoolLayer(2)\n",
        "    self.relu2 = ReluLayer()\n",
        "\n",
        "    ## 4: First fully connected layer, with additional steps\n",
        "    self.fc1 = LinearLayer(80, 50)\n",
        "    self.relu3 = ReluLayer()\n",
        "    self.dropout2 = DropoutLayer()\n",
        "\n",
        "    ## 5: Second fully connected layer, with additional steps\n",
        "    self.fc2 = LinearLayer(50, 10)\n",
        "    self.softmax = SoftMaxLayer()\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Step 1: output shape should be 64x10x14x14\n",
        "    x = self.relu1(self.maxpool1(self.conv1(x)))\n",
        "\n",
        "    # Step 2: output shape should be 64x20x5x5\n",
        "    x = self.relu2(self.maxpool2(self.dropout1(self.conv2(x))))\n",
        "\n",
        "    # Step 3: output shape should be 64x80\n",
        "    x = x.view(-1, 80)\n",
        "\n",
        "    # Step 4: output shape should be 64x50\n",
        "    x = self.dropout2(self.relu3(self.fc1(x)))\n",
        "\n",
        "    # Step 5: output shape should be 64x10\n",
        "    x = self.softmax(self.fc2(x))\n",
        "\n",
        "    # Return the output\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2LEaUhWihWM"
      },
      "source": [
        "This cell defines the loss function, instantiates the model, moves is to the GPU, and creates the optimizer just as in the last exercises."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4T12nqpFurAx"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.01\n",
        "momentum = 0.5\n",
        "\n",
        "## Initialize Cross Entropy Loss Function\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "## Initializer the model and optimizer\n",
        "model = DigitsModel()\n",
        "model = model.cuda()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate, momentum = momentum)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVocxfOjioqp"
      },
      "source": [
        "Finally, we run the standard PyTorch training loop on with the defined model, optimizer, and loss function. If the model throws errors, try to isolate which layers are causing them and carefully check the shapes, logic, and other features of the layer.\n",
        "\n",
        "We've set the model to train for only two epochs, since this model will take a while to train. With our implementation it averages 160s/epoch on the GPu, but experiences may vary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fmf9N-TCvw0F"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "n_epochs = 1\n",
        "\n",
        "for _ in range(n_epochs):\n",
        "\n",
        "  for X, y in tqdm(train_loader):\n",
        "\n",
        "    # Move X and y to the GPU\n",
        "    X = X.cuda()\n",
        "    y = y.cuda()\n",
        "\n",
        "    # Create predictions by running the model\n",
        "    preds = model(X)\n",
        "\n",
        "    # Compute the loss with the loss function defined above\n",
        "    loss = loss_fn(preds, y.long())\n",
        "\n",
        "    # Zero the optimizer grads by calling optimizer.zero_grad()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Call the `backward` function on the loss to compute the gradients\n",
        "    loss.backward()\n",
        "\n",
        "    # Call `step` on the optimzer to update the parameters\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymt2i9dAjJnx"
      },
      "source": [
        "**Finally**, check your model's preformance on the test set, again using code borrowed from the last exercise set. This model will likely preform worse than the model in the last exercises, for a variety of reasons:\n",
        "\n",
        "\n",
        "1.   The model is much smaller than the previous one\n",
        "2.   The model has been trained for fewer epochs.\n",
        "3.   The model was initialized with suboptimal weights. See the paper below for one explaination of why weights drawn from our distributions may not be optimal, and for a derivation of more optimal intial weights.\n",
        "\n",
        "However, we should still see the model learning and should observe some performance gains. After 2 epochs, performance under 50% should raise a red flag, but something between 50 and 70% is standard.\n",
        "\n",
        "We recommend training for only a single epoch. Further training is likely to result in worse preformance and substantial overfitting.\n",
        "\n",
        "K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into rectifiers:\n",
        "Surpassing human-level performance on imagenet classification. In\n",
        "ICCV, 2015."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xtl9JMHg3GE6"
      },
      "outputs": [],
      "source": [
        "n_correct = 0\n",
        "count = 0\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "  for X, y in test_loader:\n",
        "    if count > 100:\n",
        "      break\n",
        "    # Check if the model has predicted accurately\n",
        "    X = X.cuda()\n",
        "    y = y.cuda()\n",
        "    output = model(X).cpu().detach().numpy()\n",
        "    pred = np.argmax(output)\n",
        "    count += 1\n",
        "    if pred == y.item():\n",
        "      n_correct += 1\n",
        "\n",
        "print(n_correct / count)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGaep-rAtVNb"
      },
      "source": [
        "### 2. Transfer Learning with Remote Sensing Data\n",
        "\n",
        "This set of exercises introduces the `timm` library, which provides a wide variety of pretrained image models. Pretrained image models have been tuned to understand images by extensive training on a large dataset, typically ImageNet.\n",
        "\n",
        "Let's start by installing the `timm` library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EIrcShnU1qzd"
      },
      "outputs": [],
      "source": [
        "!pip install timm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k12N5wUHuyhc"
      },
      "source": [
        "And some necessary imports:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wEuA3RbpuyPp"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from PIL import Image\n",
        "import torchvision\n",
        "from matplotlib import pyplot as plt\n",
        "import timm\n",
        "from tqdm import tqdm\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vs9Sr_QEutqV"
      },
      "source": [
        "Before we start talking about models, let's bring in an interesting dataset to work with. This is the [UC Merced Satelite Landuse Dataset](http://weegee.vision.ucmerced.edu/datasets/landuse.html), which includes images from 21 categories of Landuses. Our object will be to correctly classify the various landuses shown in the images.\n",
        "\n",
        "To download the dataset and see examples of the various landuses, run the following cells."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1vT9V4jikbi2"
      },
      "outputs": [],
      "source": [
        "!wget http://weegee.vision.ucmerced.edu/datasets/UCMerced_LandUse.zip\n",
        "!unzip -q UCMerced_LandUse.zip\n",
        "os.chdir('UCMerced_LandUse')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MAoDLZLDm14M"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize = (12, 12))\n",
        "for i, cat in enumerate(os.listdir('./Images')):\n",
        "  im = Image.open(os.path.join('./Images', cat, cat + '00.tif'))\n",
        "  ax = fig.add_subplot(5, 5, i + 1)\n",
        "  ax.imshow(im)\n",
        "  ax.set_title(cat)\n",
        "\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8eHZHqpWCZQh"
      },
      "source": [
        "You should see the 21 different possible landuses, with examples of each, displayed above. (Fun fact that I learned from this, \"Chaparral\" is a kind of shrubland, primarily found in California, Oregon, and Mexico.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEMQZ_MWC1uQ"
      },
      "source": [
        "##### a) **Creating Datasets and Dataloaders**\n",
        "\n",
        "As per usual, we first need to create torch-style `torch.utils.data.Dataset` and `torch.utils.data.DataLoader` objects to load our data into a forthcoming training loop. Here we will create a train, validation, and test dataset.\n",
        "\n",
        "Use `torch.utils.data.random_split` to divide the provided dataset into three, with an 80% train, 10% validation, and 10% test split. Then create associated `DataLoader` objects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZdRtFrKF0hks"
      },
      "outputs": [],
      "source": [
        "landuse_dataset = torchvision.datasets.ImageFolder('./Images', transform=torchvision.transforms.Compose([\n",
        "                                                                    torchvision.transforms.ToTensor(),\n",
        "                                                                    torchvision.transforms.CenterCrop([256, 256])\n",
        "                                                                    ]))\n",
        "\n",
        "# TODO: Split the dataset into three with the provided sizes\n",
        "train_set, val_set, test_set = torch.utils.data.random_split(landuse_dataset, [0.8, 0.1, 0.1])\n",
        "\n",
        "train_batch_size = 16\n",
        "val_batch_size = 16\n",
        "test_batch_size = 1\n",
        "\n",
        "# TODO: Create dataloader objects\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size = train_batch_size, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_set, batch_size = val_batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_set, batch_size = test_batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwwoQ_ShDq3J"
      },
      "source": [
        "##### b) **Downloading a Pretrained Model**\n",
        "\n",
        "The `timm` library is prepared to provide us with a variety of model architectures and pretrained weights. Here is a first example, where we create a model (still a `torch.nn.Module` object) with the ConvNeXt architecture, which will classify samples into one of 21 classes (since there are 21 classes in our target dataset), and which is initialized with ImageNet pretrained weights. ConvNeXt is a powerful, modern convolutional architecture good for a wide variety of image classification, object detection, and other tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J9NYox2tm9NN"
      },
      "outputs": [],
      "source": [
        "convnext_model = timm.create_model('convnext_tiny', num_classes = 21, pretrained=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkI32q9tEX1e"
      },
      "source": [
        "##### c) **Creating a loss function and optimizer**\n",
        "\n",
        "Similar to the last exercise set, we need to create a loss function and optimizer. We will again use `CrossEntropyLoss` for the loss function, but this time we will use an Adam (`torch.optim.Adam`) optimizer.\n",
        "\n",
        "For more information on Adam, see here: https://arxiv.org/pdf/1412.6980.pdf. Adam is a specialized form of gradient descent that estimates different learning rates for different parameters based on the moments (in the mathematical sense) of their gradients. It tends to help large models converge faster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IN70GHzS5Lvh"
      },
      "outputs": [],
      "source": [
        "# TODO: Create the loss function\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "learning_rate = 1e-5\n",
        "\n",
        "# TODO: Create the ADAM optimizer\n",
        "optim = torch.optim.Adam(convnext_model.parameters(), lr = learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVzJwDWXFtEu"
      },
      "source": [
        "##### d) **Training and Validation Loop**\n",
        "\n",
        "Now that we have all the required objects, we are ready to start training! We will set up the training loop very similarly to the previous exercises, with one key addition: we add a _validation_ step, where we measure the model's progress by computing the accuracy on the validation set after each training epoch.\n",
        "\n",
        "Five epochs should let this model learn the data fairly well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4sWz7Rf940B-"
      },
      "outputs": [],
      "source": [
        "n_epochs = 5\n",
        "convnext_model = convnext_model.cuda()\n",
        "\n",
        "for i in tqdm(range(n_epochs)):\n",
        "\n",
        "  # Train Step\n",
        "  convnext_model.train()\n",
        "  epoch_loss = []\n",
        "\n",
        "  for X, y in train_loader:\n",
        "\n",
        "    # TODO: Fill in the training loop\n",
        "    X = X.cuda()\n",
        "    y = y.cuda()\n",
        "\n",
        "    optim.zero_grad()\n",
        "    preds = convnext_model(X)\n",
        "    loss = loss_fn(preds, y)\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "\n",
        "  # Eval Step\n",
        "  n_correct = 0\n",
        "  for X, y in val_loader:\n",
        "\n",
        "    # TODO: Compute the Validation Accuracy\n",
        "    X = X.cuda()\n",
        "\n",
        "    # Run the model over X to generate probabilities\n",
        "    output = convnext_model(X)\n",
        "    cpu_output = output.detach().cpu().numpy()\n",
        "    # Find the maximal probability for each x to get predictions\n",
        "    preds = np.argmax(cpu_output, axis = 1)\n",
        "    # Determine if predictions were correct\n",
        "    n_correct += (preds == y.numpy()).sum()\n",
        "\n",
        "  print(f'Epoch {i}: val accuracy: {(n_correct / len(val_set)):3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpwRwHWgMV3p"
      },
      "source": [
        "##### e) **Model Testing**\n",
        "\n",
        "Finish this section by evaluating the trained model on your test set. How does the model do? Test accuracy should be close to final validation accuracy, and both should be close to 90% for this problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q24na9Vi7DUy"
      },
      "outputs": [],
      "source": [
        "convnext_model.eval()\n",
        "n_correct = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "  for X, y in test_loader:\n",
        "    # Check if the model has predicted accurately\n",
        "    X = X.cuda()\n",
        "    y = y.cuda()\n",
        "    output = convnext_model(X).cpu().detach().numpy()\n",
        "    pred = np.argmax(output, axis = 1)\n",
        "\n",
        "    n_correct += (pred == y.cpu().detach().numpy()).sum()\n",
        "\n",
        "print(f'Test accuracy: {(n_correct / len(test_set)):3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuXiBrjrMsRG"
      },
      "source": [
        "##### f) **Continued Exploration**\n",
        "\n",
        "Follow a similar process to train a series of other models on this dataset. For each model, notice how it learns compared to this first ConvNeXt model. Why do you think they learn faster or slower?\n",
        "\n",
        "Start by training the same `convnext_tiny` model, but without pretrained weights. How does the model do on the validation set? On the training set? How does pretraining account for the difference?\n",
        "\n",
        "Next, train at least two other models from `timm`. To see a list of available models, you can run `timm.list_models(pretrained=True)`\n",
        "\n",
        "Suggested, interesting models could include:\n",
        "\n",
        "\n",
        "*   ResNet, in one or more of its forms\n",
        "*   MobileNetv3\n",
        "*   VGG16 or VGG19\n",
        "*   A vision transformer model like vit or swin\n",
        "\n",
        "You need to experiment with various model hyperparameters to find ones that allow your models to train quickly. Some models are particularly sensitive to the learning rate. When you have finished, write a few sentences about what you've noticed while training the various models.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "ISwhhORYyBYA"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}